import time
import random
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# âœ… í¬ë¡¬ ì˜µì…˜ ì„¤ì •
chrome_options = Options()
chrome_options.headless = False  # í•„ìš” ì‹œ Trueë¡œ ë³€ê²½ ê°€ëŠ¥
chrome_options.add_argument("--disable-blink-features=AutomationControlled")
chrome_options.add_argument("--disable-infobars")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--start-maximized")
chrome_options.page_load_strategy = "eager"
chrome_options.add_argument(
    "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
)
chrome_options.add_experimental_option("detach", True)

# âœ… ì›¹ë“œë¼ì´ë²„ ì‹¤í–‰
driver = webdriver.Chrome(options=chrome_options)
driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

# âœ… ê°•ë‚¨ì–¸ë‹ˆ ì¹´ì¹´ì˜¤ ë¡œê·¸ì¸ URL
login_url = "https://accounts.kakao.com/login/?continue=https%3A%2F%2Fkauth.kakao.com%2Foauth%2Fauthorize%3Fclient_id%3D36cf3898c3072e555ea6a49b299f8a06%26redirect_uri%3Dhttps%253A%252F%252Fwww.gangnamunni.com%252Fsignup%252Foauth%252Fkakao%26response_type%3Dcode%26scope%3Daccount_email%252Cbirthday%252Cname%252Cphone_number%252Ctalk_message%26through_account%3Dtrue#login"
driver.get(login_url)

# âœ… ì‚¬ìš©ìê°€ ìˆ˜ë™ìœ¼ë¡œ ë¡œê·¸ì¸í•  ì‹œê°„ ì œê³µ
print("ğŸ” ë¡œê·¸ì¸í•˜ì„¸ìš”. ë¡œê·¸ì¸ í›„ 70ì´ˆ ë‚´ë¡œ í˜ì´ì§€ê°€ ìë™ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤...")
try:
    WebDriverWait(driver, 90).until(
        EC.presence_of_element_located((By.CLASS_NAME, "GlobalHeader__StyledUserImage-sc-35f0e887-9"))
    )
    print("âœ… ë¡œê·¸ì¸ ì„±ê³µ! í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
except:
    print("âš ï¸ ë¡œê·¸ì¸ ì‹¤íŒ¨! ìˆ˜ë™ ë¡œê·¸ì¸ í™•ì¸ í•„ìš”.")

# âœ… í¬ë¡¤ë§í•  ë³‘ì› ë¦¬ë·° í˜ì´ì§€ URL
page_url = "https://www.gangnamunni.com/reviews?hospitalId=2991"
driver.get(page_url)
time.sleep(5)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

# âœ… "ë”ë³´ê¸°" ë²„íŠ¼ì´ ì¡´ì¬í•˜ë©´ í´ë¦­ (ë°˜ë³µ ì‹¤í–‰)
def click_more_button():
    """ë”ë³´ê¸° ë²„íŠ¼ì´ ì¡´ì¬í•˜ë©´ ê³„ì† í´ë¦­í•˜ì—¬ ë¦¬ë·° ì¶”ê°€ ë¡œë“œ"""
    while True:
        try:
            more_button = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, "//button[span[contains(text(), 'ë”ë³´ê¸°')]]"))
            )
            driver.execute_script("arguments[0].click();", more_button)
            print("âœ… 'ë”ë³´ê¸°' ë²„íŠ¼ í´ë¦­ë¨")
            time.sleep(random.uniform(2, 4))
        except:
            print("ğŸ“Œ 'ë”ë³´ê¸°' ë²„íŠ¼ì´ ë” ì´ìƒ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
            break  # ë” ì´ìƒ ë²„íŠ¼ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ

# âœ… ìŠ¤í¬ë¡¤ ë‹¤ìš´í•˜ì—¬ ìƒˆë¡œìš´ ë¦¬ë·° ë¡œë“œ
def scroll_to_bottom():
    """ìŠ¤í¬ë¡¤ì„ ëê¹Œì§€ ë‚´ë ¤ ìƒˆë¡œìš´ ë¦¬ë·° ë¡œë“œ"""
    last_height = driver.execute_script("return document.body.scrollHeight")
    
    cnt = 1
    while True: 
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(random.uniform(2, 3))
        
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            print("ğŸš€ ëª¨ë“  ë¦¬ë·° í¬ë¡¤ë§ ì™„ë£Œ! ë” ì´ìƒ ì¶”ê°€ ë¡œë“œí•  ë¦¬ë·° ì—†ìŒ.")
            break
        last_height = new_height        
        if cnt >= 6000:
            break    
        else :
            cnt += 1 

# âœ… "ë”ë³´ê¸°" ë²„íŠ¼ í´ë¦­ ì‹¤í–‰
click_more_button()

# âœ… ìŠ¤í¬ë¡¤ ë‹¤ìš´ ì‹¤í–‰
scroll_to_bottom()

# âœ… ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
reviews_data = []

def scrape_reviews():
    """í˜„ì¬ í˜ì´ì§€ì—ì„œ ë¦¬ë·° ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜"""
    review_elements = driver.find_elements(By.CSS_SELECTOR, "div.new__StyledContainer-sc-1ee6cb43-0")

    print(f"ğŸ” {len(review_elements)}ê°œì˜ ë¦¬ë·° ê°ì§€ë¨")

    for review in review_elements:
        try:
            # âœ… ì‘ì„±ì ì •ë³´
            author = review.find_element(By.CSS_SELECTOR, "h3.UserProfile__StyledName-sc-36315857-4").text.strip()

            # âœ… ì‘ì„±ì ë ˆë²¨ (ì˜ˆ: Lv.1, Lv.2)
            try:
                level = review.find_element(By.CSS_SELECTOR, "span.UserProfile__StyeldLevel-sc-36315857-5").text.strip()
            except:
                level = "ë ˆë²¨ ì—†ìŒ"

            # âœ… ë¦¬ë·° ë‚ ì§œ
            date = review.find_element(By.CSS_SELECTOR, "span.new__StyledDate-sc-1ee6cb43-4").text.strip()

            # âœ… í‰ì 
            # rating = review.find_element(By.CSS_SELECTOR, "span.new__StyledRating-sc-1ee6cb43-7").text.strip()
            try:
                rating = review.find_element(By.CSS_SELECTOR, "span[class*='Rating']").text.strip()
            except:
                rating = "í‰ì  ì—†ìŒ"
            # âœ… í•´ì‹œíƒœê·¸ (ì‹œìˆ ëª…)
            try:
                hashtags = review.find_element(By.CSS_SELECTOR, "p.new__StyledTreatmentTagList-sc-1ee6cb43-8").text.strip()
            except:
                hashtags = "íƒœê·¸ ì—†ìŒ"

            # âœ… ë¦¬ë·° ë³¸ë¬¸
            try:
                content = review.find_element(By.CSS_SELECTOR, "p.new__StyledDescription-sc-1ee6cb43-10").text.strip()
            except:
                content = "ë‚´ìš© ì—†ìŒ"

            # âœ… ë°ì´í„° ì €ì¥
            reviews_data.append([author, level, date, rating, hashtags, content]) 
             

        except Exception as e:
            print(f"âŒ ë¦¬ë·° í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

# âœ… ë¦¬ë·° í¬ë¡¤ë§ ì‹¤í–‰
scrape_reviews()

# âœ… í¬ë¡¤ë§ëœ ë°ì´í„° ì €ì¥
df = pd.DataFrame(reviews_data, columns=["ì‘ì„±ì", "ë ˆë²¨", "ì‘ì„± ë‚ ì§œ", "í‰ì ", "íƒœê·¸", "ë‚´ìš©"])
df.to_csv("gangnamunni_reviews.csv", index=False, encoding="utf-8-sig")

print("âœ… í¬ë¡¤ë§ ì™„ë£Œ! ë°ì´í„°ê°€ 'gangnamunni_reviews.csv' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# âœ… ë“œë¼ì´ë²„ ì¢…ë£Œ
driver.quit()
